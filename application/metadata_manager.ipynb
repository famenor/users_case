{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1501ced-9060-473a-b1a0-a93a90a7173b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from lib.gateway.database import SparkSQLDatabaseGateway\n",
    "from lib.interactor.governance import GovernanceInteractor\n",
    "from lib.interactor.asset import AssetInteractor\n",
    "from lib.interactor.surrogate_key import SurrogateKeyInteractor\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c6cb15-8348-4bed-a652-679db71d913e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def propagate_raw_to_audit(target_layer, source_layer):\n",
    "\n",
    "    for column_name in target_layer['schema'].keys():\n",
    "        \n",
    "        if 'rename_from' in target_layer['schema'][column_name].keys():\n",
    "            rename_from = target_layer['schema'][column_name]['rename_from']\n",
    "        else:\n",
    "            rename_from = column_name\n",
    "            target_layer['schema'][column_name]['rename_from'] = rename_from\n",
    "\n",
    "        column = target_layer['schema'][column_name]\n",
    "        for fill_column in ['data_type', 'is_primary_key', 'is_nullable', 'is_partition', 'is_pii', 'comment']:\n",
    "            if fill_column not in column.keys() and fill_column in source_layer['schema'][rename_from].keys():\n",
    "                target_layer['schema'][column_name][fill_column] = source_layer['schema'][rename_from][fill_column]\n",
    "\n",
    "    return target_layer\n",
    "\n",
    "def propagate_audit_to_historic(target_layer, source_layer):\n",
    "\n",
    "    for column_name in target_layer['schema'].keys():\n",
    "\n",
    "        rename_from = column_name\n",
    "        target_layer['schema'][column_name]['rename_from'] = rename_from\n",
    "\n",
    "        column = target_layer['schema'][column_name]\n",
    "        for fill_column in ['data_type', 'is_primary_key', 'is_nullable', 'is_partition', 'is_pii', 'comment']:\n",
    "            if fill_column not in column.keys() and fill_column in source_layer['schema'][rename_from].keys():\n",
    "                target_layer['schema'][column_name][fill_column] = source_layer['schema'][rename_from][fill_column]\n",
    "\n",
    "    return target_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26a7d51-ea9d-438c-9ced-1a68e321cdaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database_gateway = SparkSQLDatabaseGateway()\n",
    "governance_interactor = GovernanceInteractor(database_gateway=database_gateway)\n",
    "asset_interactor = AssetInteractor(database_gateway=database_gateway)\n",
    "surrogate_key_interactor = SurrogateKeyInteractor(database_gateway=database_gateway)\n",
    "\n",
    "schema_table_detail = StructType([\n",
    "    StructField('table_id', IntegerType(), False),\n",
    "    StructField('column_id', IntegerType(), False),\n",
    "    StructField('column_name', StringType(), False),\n",
    "    StructField('rename_from', StringType(), True),\n",
    "    StructField('data_type', StringType(), False),\n",
    "    StructField('ordinal_position', IntegerType(), False),\n",
    "    StructField('is_primary_key', BooleanType(), False),\n",
    "    StructField('is_nullable', BooleanType(), False),\n",
    "    StructField('is_partition', BooleanType(), False),\n",
    "    StructField('is_pii', BooleanType(), False),\n",
    "    StructField('validations', StringType(), True),\n",
    "    StructField('track_changes', BooleanType(), True),\n",
    "    StructField('comment', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad253fa-2ed9-4adf-a8d2-8cd3c9151bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_base = '/Workspace/Users/armando.n90@gmail.com/users_case/lakehouse/governance/metadata/assets/'\n",
    "domain = 'domain'\n",
    "environment = 'dev'\n",
    "source = 'analytics'\n",
    "plain_tables = ['visitas']\n",
    "plain_table = plain_tables[0]\n",
    "   \n",
    "print('Processing plain table:', plain_table)\n",
    "path = f'{path_base}/{domain}/{source}/{plain_table}.yml'\n",
    "\n",
    "with open(path) as file:\n",
    "    try:\n",
    "        metadata = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "name = metadata['name']\n",
    "raw_layer = None\n",
    "audit_layer = None\n",
    "historic_layer = None\n",
    "\n",
    "if 'raw' in metadata['layers'].keys():\n",
    "    raw_layer = metadata['layers']['raw']\n",
    "    raw_layer['layer'] = 'raw'\n",
    "\n",
    "if 'audit' in metadata['layers'].keys():\n",
    "    audit_layer = metadata['layers']['audit']\n",
    "    audit_layer['layer'] = 'audit'\n",
    "\n",
    "if 'historic' in metadata['layers'].keys():\n",
    "    historic_layer = metadata['layers']['historic']\n",
    "    historic_layer['layer'] = 'historic'\n",
    "\n",
    "audit_layer = propagate_raw_to_audit(audit_layer, raw_layer)\n",
    "historic_layer = propagate_audit_to_historic(historic_layer, audit_layer)\n",
    "\n",
    "for layer in [raw_layer, audit_layer, historic_layer]:\n",
    "\n",
    "    layer_name = layer['layer']\n",
    "    print('Processing layer: ' + layer_name)\n",
    "\n",
    "    catalog_name = f'{domain}_{environment}'\n",
    "    table_name = layer_name +  '_' + name\n",
    "    print('Ingesting metadata table: ' + table_name)\n",
    "    \n",
    "    etl_module = None\n",
    "    schema_name = None\n",
    "    quality = None\n",
    "    table_type = None\n",
    "\n",
    "    if layer_name == 'raw':\n",
    "        etl_module = metadata['name'] + '__load'\n",
    "        schema_name = f'bronze_{source}'\n",
    "        quality = 'bronze'\n",
    "        table_type = 'table'\n",
    "\n",
    "    elif layer_name == 'audit':\n",
    "        etl_module = metadata['name'] + '__audit'\n",
    "        schema_name = f'silver_{source}'\n",
    "        quality = 'silver'\n",
    "        table_type = 'table'\n",
    "\n",
    "    elif layer_name == 'historic':\n",
    "        etl_module = metadata['name'] + '__hist'\n",
    "        schema_name = f'silver_{source}'\n",
    "        quality = 'silver'\n",
    "        table_type = 'table'\n",
    "\n",
    "    catalog_id = governance_interactor.get_catalog_id(catalog_name)\n",
    "    schema_id = governance_interactor.get_schema_id(catalog_id, schema_name)\n",
    "\n",
    "    base_dict = {'schema_id': schema_id, 'table_name': table_name, 'version': 1}\n",
    "    table_id = surrogate_key_interactor.assign_surrogate_key(catalog_name='governance_prod', schema_name='metadata', \n",
    "                                                                table_name='tables', base_values=base_dict, surrogate_column='table_id')\n",
    "\n",
    "    write_mode = layer['write_mode']\n",
    "    version = layer['version']\n",
    "    current_flag = True\n",
    "    valid_from = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S') \n",
    "    valid_to = '2200-01-01 00:00:00'\n",
    "    description = layer['description']\n",
    "    owner = layer['owner']\n",
    "    retention_policy = layer['retention_policy']\n",
    "\n",
    "    columns = ['table_id', 'schema_id', 'table_name', 'etl_module', 'write_mode', 'quality', \n",
    "            'table_type', 'version', 'current_flag', 'valid_from', 'valid_to', 'description', \n",
    "            'owner', 'retention_policy']\n",
    "    \n",
    "    ingestion = [(table_id, schema_id, table_name, etl_module, write_mode, quality, \n",
    "                table_type, version, current_flag, valid_from, valid_to, description, \n",
    "                owner, retention_policy)]\n",
    "\n",
    "    dataframe_ingestion = spark.createDataFrame(ingestion, columns)\n",
    "    asset_interactor.merge_dataframe(dataframe_ingestion, catalog_name='governance_prod', schema_name='metadata', \n",
    "                                        table_name='tables', match_columns=['table_id'])\n",
    "\n",
    "    i = 1\n",
    "    for column_name in layer['schema'].keys():\n",
    "\n",
    "        base_dict = {'table_id': table_id, 'column_name': column_name}\n",
    "        print('Ingesting metadata table details for column: ' + column_name)\n",
    "\n",
    "        column = layer['schema'][column_name]        \n",
    "        column_id = surrogate_key_interactor.assign_surrogate_key(catalog_name='governance_prod', \n",
    "                                                            schema_name='metadata', \n",
    "                                                            table_name='tables_detail', \n",
    "                                                            base_values=base_dict, \n",
    "                                                            surrogate_column='column_id')\n",
    "        \n",
    "        is_primary_key = False\n",
    "        rename_from = None\n",
    "        validations = None\n",
    "        is_pii = False\n",
    "        track_changes = None\n",
    "\n",
    "        if 'is_primary_key' in column.keys():\n",
    "            is_primary_key = column['is_primary_key']\n",
    "\n",
    "        if 'rename_from' in column.keys():\n",
    "            rename_from = column['rename_from']\n",
    "\n",
    "        if 'validations' in column.keys():\n",
    "            validations = json.dumps(column['validations'])\n",
    "\n",
    "        if 'is_pii' in column.keys():\n",
    "            is_pii = column['is_pii']\n",
    "\n",
    "        if 'track_changes' in column.keys():\n",
    "            track_changes = column['track_changes']\n",
    "                \n",
    "        ingestion = [(table_id, column_id, column_name, rename_from, column['data_type'], i, \n",
    "                     is_primary_key, column['is_nullable'], column['is_partition'], is_pii, \n",
    "                     validations, track_changes, column['comment'])]\n",
    "\n",
    "        dataframe_ingestion = spark.createDataFrame(ingestion, schema_table_detail)\n",
    "\n",
    "        asset_interactor.merge_dataframe(dataframe_ingestion, catalog_name='governance_prod', schema_name='metadata',\n",
    "                                            table_name='tables_detail', match_columns=['column_id'])\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84559aff-59dc-4766-a769-3a486a38ee37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = spark.sql(f\"select * from governance_prod.metadata.tables\")\n",
    "test.show(100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fba0b2-58e3-413d-a89b-977b4a11862b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = spark.sql(f\"select * from governance_prod.metadata.tables_detail WHERE table_id = 1\")\n",
    "test.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4285db-8ac4-40bc-b4ca-4c0360cdb816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = spark.sql(f\"select * from governance_prod.metadata.tables_detail WHERE table_id = 2\")\n",
    "test.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6961fc-4104-43aa-8705-333e6e5bf680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = spark.sql(f\"select * from governance_prod.metadata.tables_detail WHERE table_id = 3\")\n",
    "test.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c42975-b4b2-4a3d-a71d-fe0bc40df2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql('DELETE FROM governance_prod.metadata.tables')\n",
    "#spark.sql('DELETE FROM governance_prod.metadata.tables_detail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c363bf3b-67d6-497c-98b7-c72e946861b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "metadata_manager",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

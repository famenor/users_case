{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5b98e8-5d0e-4287-8ce4-703608c61a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182d4ecc-5dcf-4300-9c4f-f5a2441e908b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import json\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from lib.gateway.file import *\n",
    "from lib.gateway.database import *\n",
    "from lib.interactor.governance import *\n",
    "from lib.interactor.asset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2313d8e0-e41c-4c68-9f24-1c4b77ce29c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## FACTORIES FOR COMPONENTS REQUIRED BY THE TEMPLATE\n",
    "\n",
    "#ABTRACT FACTORY WITH METHODS TO BE IMPLEMENTED\n",
    "class AbstractFactoryRawToAudit(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def create(self):\n",
    "        pass\n",
    "\n",
    "#CONCRETE FACTORY FOR TEMPLATE BASED ON CSV FILE WITH COMMA SEPARATOR\n",
    "class FactoryRawToAudit(AbstractFactoryRawToAudit):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def create(self):\n",
    "\n",
    "        database_gateway = SparkSQLDatabaseGateway()\n",
    "        #self.table_reader = CsvFileReader(file_path=self.path)\n",
    "  \n",
    "        self.governance_interactor = GovernanceInteractor(database_gateway=database_gateway)\n",
    "        self.asset_interactor = AssetInteractor(database_gateway=database_gateway)\n",
    "     \n",
    "\n",
    "class Demo(ABC):\n",
    "\n",
    "    def get_is_not_null_failures(self, catalog_name: str, schema_name: str, table_name: str, column_name: str):\n",
    "        pass\n",
    "\n",
    "    def get_is_unique_failures(self, catalog_name: str, schema_name: str, table_name: str, column_names: list):\n",
    "        pass\n",
    "\n",
    "    def get_is_in_bounds_failures(self, catalog_name: str, schema_name: str, table_name: str, column_name: str, \n",
    "                                  min_value: object, max_value: object, data_type: str):\n",
    "        pass\n",
    "\n",
    "    def get_is_in_list_failures(self, catalog_name: str, schema_name: str, table_name: str, column_name: str, list_values: list):\n",
    "        pass\n",
    "\n",
    "    def get_is_date_format_failures(self, catalog_name: str, schema_name: str, table_name: str, column_name: str, date_format: str):\n",
    "        pass\n",
    "\n",
    "    def get_is_not_lower_than_failures(self, catalog_name: str, schema_name: str, table_name: str, column_name: str, reference_column_name: str):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559a0d65-86de-4ebb-b9f1-1d5ebcc41e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## TEMPLATE FOR EXTRACTING A TABLE AND METADATA, PROCESSING THE TABLE AND EXPORTING THE TABLE WITH METADATA AND METRICS\n",
    "\n",
    "#INTERFACE FOR THE TEMPLETE\n",
    "class InterfaceRawToAuditTemplate(ABC):\n",
    "\n",
    "    #@abstractmethod\n",
    "    #def set_component_factory(self, component_factory: AbstractFactoryLandToRaw):\n",
    "    #    pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "#IMPLEMENTATION FOR THE TEMPLATE\n",
    "class RawToAuditTemplate(InterfaceRawToAuditTemplate):\n",
    "\n",
    "    def __init__(self, catalog_name: str, schema_name: str, table_name: str, batch_id: str):\n",
    "\n",
    "        self.catalog_name = catalog_name\n",
    "        self.table_name = table_name\n",
    "        self.schema_name = schema_name\n",
    "        self.batch_id = batch_id\n",
    "\n",
    "        #self.table_reader = None\n",
    "        self.governance_interactor = None\n",
    "        self.asset_interactor = None\n",
    "\n",
    "        self.dataframe = None\n",
    "        self.metadata = None\n",
    "        \n",
    "        self.metrics = {}\n",
    "        self.metrics['batch_id'] = batch_id\n",
    "        self.metrics['loaded_at'] = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.metrics['initial_rows'] = None\n",
    "        self.metrics['final_rows'] = None\n",
    "        self.metrics['accumulated_rows'] = None\n",
    "\n",
    "        self.dataframe_ingestion_metrics = None\n",
    "\n",
    "    def set_component_factory(self, component_factory: AbstractFactoryRawToAudit):\n",
    "    #    self.table_reader = component_factory.table_reader\n",
    "        self.governance_interactor = component_factory.governance_interactor\n",
    "        self.asset_interactor = component_factory.asset_interactor\n",
    "  \n",
    "    def process(self):\n",
    "\n",
    "        print('Starting land to raw process ...')\n",
    "\n",
    "        #EXTRACTION STEPS\n",
    "        self.extract_metadata()\n",
    "        self.extract_table()\n",
    "\n",
    "        #PROCESSING STEPS\n",
    "        self.rename_columns()\n",
    "        #self.validate_required_columns()\n",
    "        #self.cast_data_types()\n",
    "        #self.validate_non_nullable_columns()\n",
    "        #self.add_metadata()\n",
    "\n",
    "        #EXPORTING STEPS\n",
    "        #self.write_table()\n",
    "        #self.get_accumulated_rows()\n",
    "        #self.generate_ingestion_metrics()\n",
    "        #self.write_metrics()\n",
    "\n",
    "        print('Table, metadata and ingestion metrics exported successfully')\n",
    "\n",
    "    #EXTRACT METADATA\n",
    "    def extract_metadata(self):\n",
    "     \n",
    "        print('Extracting metadata ...')\n",
    "        self.metadata = self.governance_interactor.get_table_metadata(catalog_name=self.catalog_name, \n",
    "                                                                      schema_name=self.schema_name, \n",
    "                                                                      table_name=self.table_name)\n",
    "\n",
    "    #EXTRACT TABLE\n",
    "    def extract_table(self):\n",
    "\n",
    "        #EXTRACT LANDING FILE\n",
    "        print('Extracting landing file ...')\n",
    "\n",
    "        raw_schema_name = self.schema_name.replace('silver', 'bronze')\n",
    "        raw_table_name = self.table_name.replace('audit', 'raw')\n",
    "        params = {'read_mode': 'read_partition', 'batch_id': self.batch_id}\n",
    "\n",
    "        self.dataframe = self.asset_interactor.read_table(catalog_name=self.catalog_name, \n",
    "                                                          schema_name=raw_schema_name, \n",
    "                                                          table_name=raw_table_name, \n",
    "                                                          params=params)\n",
    "        \n",
    "        self.dataframe = self.dataframe.drop('metadata_batch_id', 'metadata_loaded_at', 'metadata_etl_module')\n",
    "        self.dataframe = self.dataframe.withColumn('created_at', lit(None))\n",
    "        self.dataframe = self.dataframe.withColumn('updated_at', lit(None))\n",
    "        self.dataframe = self.dataframe.withColumn('deleted_at', lit(None))\n",
    "        self.dataframe = self.dataframe.withColumn('audit_id', lit(None))\n",
    "        self.dataframe = self.dataframe.withColumn('audit_passed', lit(True))\n",
    "        self.dataframe = self.dataframe.withColumn('row_temp_id', monotonically_increasing_id())\n",
    "      \n",
    "        self.metrics['initial_rows'] = self.dataframe.count()\n",
    "\n",
    "    #RENAME COLUMNS\n",
    "    def rename_columns(self):\n",
    "\n",
    "        print('Renaming columns ...')\n",
    "        for column in self.metadata['field_data'].keys():\n",
    "            \n",
    "            rename_from = table_processor.metadata['field_data'][column]['rename_from']\n",
    "            self.dataframe = self.dataframe.withColumnRenamed(rename_from, column)\n",
    "\n",
    "    #VALIDATE REQUIRED COLUMNS\n",
    "    def validate_required_columns(self):\n",
    "\n",
    "        print('Validating required columns ...')\n",
    "        required_columns = self.metadata['field_data'].keys()\n",
    "        dataframe_columns = self.dataframe.columns\n",
    "\n",
    "        for column in required_columns:\n",
    "            if column not in dataframe_columns:\n",
    "                raise ValueError(f\"Missing column in dataframe: {column}\")\n",
    "            \n",
    "        for column in dataframe_columns:\n",
    "            if column not in required_columns:\n",
    "                raise ValueError(f\"Missing column in metadata: {column}\")\n",
    "\n",
    "    #CAST DATA TYPES\n",
    "    def cast_data_types(self):\n",
    "        \n",
    "        print('Casting data types ...')\n",
    "        for column in self.metadata['field_data'].keys():\n",
    "\n",
    "            plain_data_type = self.metadata['field_data'][column]['data_type']\n",
    "            data_type = None\n",
    "\n",
    "            if plain_data_type == 'string':\n",
    "                data_type = StringType()\n",
    "            elif plain_data_type == 'json':\n",
    "                data_type = StringType()\n",
    "            elif plain_data_type == 'timestamp':\n",
    "                data_type = TimestampType()\n",
    "            elif plain_data_type == 'integer':\n",
    "                data_type = IntegerType()\n",
    "            elif plain_data_type == 'boolean':\n",
    "                data_type = BooleanType()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid data type: {plain_data_type}\")\n",
    "\n",
    "            self.dataframe = self.dataframe.withColumn(column, self.dataframe[column].cast(data_type))\n",
    "\n",
    "    #VALIDATE NON NULLABLE COLUMNS\n",
    "    def validate_non_nullable_columns(self):\n",
    "\n",
    "        print('Validating non nullable columns ...')\n",
    "        for column in self.metadata['field_data'].keys():\n",
    "\n",
    "            #NON NULLABLE\n",
    "            if self.metadata['field_data'][column]['is_nullable'] == False:\n",
    "                count_null = self.dataframe.filter(col(column).isNull()).count()\n",
    "\n",
    "                if count_null > 0:\n",
    "                    raise ValueError(f\"Null value found in column: {column}\") \n",
    "\n",
    "    #ADDING METADATA\n",
    "    def add_metadata(self):\n",
    "\n",
    "        #ADD COMMENTS\n",
    "        print('Adding metadata ...')\n",
    "        for column in self.metadata['field_data'].keys():\n",
    "            self.dataframe = self.dataframe.withMetadata(column, {'comment': self.metadata['field_data'][column]['comment']})\n",
    "\n",
    "        #ADD METADATA COLUMNS\n",
    "        self.dataframe = self.dataframe.withColumn('metadata_batch_id', lit(self.metrics['batch_id']))\n",
    "        self.dataframe = self.dataframe.withColumn('metadata_loaded_at', lit(self.metrics['loaded_at']))\n",
    "        self.dataframe = self.dataframe.withColumn('metadata_etl_module', lit(self.metadata['etl_module']))\n",
    "\n",
    "        self.dataframe = self.dataframe.withColumn('metadata_loaded_at', to_timestamp(col('metadata_loaded_at'), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "        self.dataframe = self.dataframe.withMetadata('metadata_batch_id', {'comment': 'Identifier of the batch process'})\n",
    "        self.dataframe = self.dataframe.withMetadata('metadata_loaded_at', {'comment': 'Timestamp when the record was loaded into the table'})\n",
    "        self.dataframe = self.dataframe.withMetadata('metadata_etl_module', {'comment': 'ETL module used to process this record'}) \n",
    "\n",
    "        self.metrics['final_rows'] = self.dataframe.count()\n",
    "\n",
    "    #WRITE TABLE\n",
    "    def write_table(self):\n",
    "\n",
    "        print('Writing table ...')\n",
    "        params = {'write_mode': self.metadata['write_mode'], 'batch_id': self.metrics['batch_id']}\n",
    "        \n",
    "        self.asset_interactor.write_table(dataframe=self.dataframe, \n",
    "                                          catalog_name=self.metadata['catalog_name'], \n",
    "                                          schema_name=self.metadata['schema_name'],\n",
    "                                          table_name=self.metadata['table_name'],\n",
    "                                          params = params)\n",
    "\n",
    "    #GET ACCUMULATED ROWS\n",
    "    def get_accumulated_rows(self):\n",
    "\n",
    "        print('Computing accumulated rows for metrics ...')\n",
    "        self.metrics['accumulated_rows'] = self.asset_interactor.get_total_rows(catalog_name=self.metadata['catalog_name'], \n",
    "                                                                                schema_name=self.metadata['schema_name'], \n",
    "                                                                                table_name=self.metadata['table_name'])\n",
    "\n",
    "    #GENERATE INGESTION METRICS\n",
    "    def generate_ingestion_metrics(self):\n",
    "\n",
    "        print('Generating ingestion metrics ...')\n",
    "        columns = ['table_id', 'catalog_name', 'schema_name', 'table_name', 'batch_id', 'loaded_at', \n",
    "                   'etl_module', 'write_mode', 'initial_rows', 'final_rows', 'accumulated_rows', 'quality']\n",
    "\n",
    "        ingestion_metrics = [(self.metadata['table_id'],\n",
    "                              self.metadata['catalog_name'], \n",
    "                              self.metadata['schema_name'], \n",
    "                              self.metadata['table_name'], \n",
    "                              self.metrics['batch_id'], \n",
    "                              self.metrics['loaded_at'], \n",
    "                              self.metadata['etl_module'], \n",
    "                              self.metadata['write_mode'], \n",
    "                              self.metrics['initial_rows'], \n",
    "                              self.metrics['final_rows'], \n",
    "                              self.metrics['accumulated_rows'],\n",
    "                              self.metadata['quality'])]\n",
    "\n",
    "        self.dataframe_ingestion_metrics = spark.createDataFrame(ingestion_metrics, columns)\n",
    "\n",
    "    #WRITE INGESTION METRICS\n",
    "    def write_metrics(self):\n",
    "\n",
    "        print('Writing ingestion metrics ...')\n",
    "        self.asset_interactor.merge_dataframe(self.dataframe_ingestion_metrics, catalog_name='governance_prod', \n",
    "                                              schema_name='metrics', table_name='ingestions', \n",
    "                                              match_columns=['catalog_name', 'schema_name', 'table_name', 'batch_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aaaf56a-582e-4531-87d2-d36fbb4e50f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "domain = 'domain'\n",
    "environment = 'dev'\n",
    "\n",
    "schema_name = 'silver_analytics'\n",
    "catalog_name = domain + '_' + environment\n",
    "\n",
    "table_name = 'audit_visitas'\n",
    "batch_id = '7'\n",
    "\n",
    "print('Processing ' + table_name + ' with batch ' + batch_id)\n",
    "factory_raw_to_audit = FactoryRawToAudit()\n",
    "factory_raw_to_audit.create()\n",
    "\n",
    "table_processor = RawToAuditTemplate(catalog_name=catalog_name, schema_name=schema_name, table_name=table_name, batch_id=batch_id)  \n",
    "table_processor.set_component_factory(factory_raw_to_audit) \n",
    "table_processor.process()\n",
    "\n",
    "print('')\n",
    "print(table_processor.metadata)\n",
    "print(table_processor.metrics)\n",
    "print('')\n",
    "table_processor.dataframe.show(3)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d64abcd-7840-4474-a9ad-160989272bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_validations = {}\n",
    "parsed_validations['is_not_null'] = []\n",
    "parsed_validations['unique'] = {}\n",
    "parsed_validations['is_in_list'] = {}\n",
    "parsed_validations['is_in_foreign_column'] = {}\n",
    "parsed_validations['is_date_format'] = {}\n",
    "parsed_validations['is_email_format'] = []\n",
    "parsed_validations['is_in_bounds'] = {}\n",
    "parsed_validations['is_not_lower_than'] = {}\n",
    "\n",
    "for column in table_processor.metadata['field_data'].keys():\n",
    "    \n",
    "    validations = table_processor.metadata['field_data'][column]['validations']\n",
    "    data_type = table_processor.metadata['field_data'][column]['data_type']\n",
    "\n",
    "    print(column, data_type)\n",
    "    if pd.isnull(validations):\n",
    "        continue\n",
    "\n",
    "    validations = json.loads(validations)\n",
    "\n",
    "    for i in range(0, len(validations)):\n",
    "        print(validations[i])\n",
    "\n",
    "        if validations[i]['validation'] == 'is_not_null':\n",
    "            parsed_validations['is_not_null'].append(column)\n",
    "\n",
    "        elif validations[i]['validation'] == 'unique':\n",
    "            unique_group = validations[i]['unique_group']\n",
    "\n",
    "            if unique_group in parsed_validations['unique'].keys():\n",
    "                parsed_validations['unique'][unique_group].append(column)\n",
    "            else:\n",
    "                parsed_validations['unique'][unique_group] = [column]\n",
    "\n",
    "        elif validations[i]['validation'] == 'is_in_list':\n",
    "\n",
    "            if 'allowed' in validations[i].keys():\n",
    "                parsed_validations['is_in_list'][column] = validations[i]['allowed'].split(',')\n",
    "\n",
    "            elif 'reference' in validations[i].keys():\n",
    "                parsed_validations['is_in_foreign_column'][column] = {'schema': validations[i]['schema'], \n",
    "                                                                      'table': validations[i]['table'], \n",
    "                                                                      'reference': validations[i]['reference']}\n",
    "                \n",
    "        elif validations[i]['validation'] == 'is_date_format':\n",
    "            parsed_validations['is_date_format'][column] = validations[i]['format']\n",
    "\n",
    "        elif validations[i]['validation'] == 'is_email_format':\n",
    "            parsed_validations['is_email_format'].append(column)\n",
    "\n",
    "        elif validations[i]['validation'] == 'is_in_bounds':\n",
    "            parsed_validations['is_in_bounds'][column] = {'min_allowed': validations[i]['min_allowed'], \n",
    "                                                          'max_allowed': validations[i]['max_allowed'],\n",
    "                                                          'data_type': data_type}\n",
    "            \n",
    "        elif validations[i]['validation'] == 'is_not_lower_than':\n",
    "            parsed_validations['is_not_lower_than'][column] = {'reference': validations[i]['reference'],\n",
    "                                                               'data_type': data_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe77f12-d517-4c38-8985-744e8ecb49e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for key in parsed_validations.keys():\n",
    "    print(key)\n",
    "    print(parsed_validations[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83df0ea-445c-46af-b2e9-479af8e94db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def conver_to_json_string(text):\n",
    "\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "\n",
    "    elements = text.split(',')\n",
    "    for element in elements:\n",
    "        if pd.isnull(element) or element == '':\n",
    "            elements.remove(element)\n",
    "    return json.dumps(elements)\n",
    "\n",
    "conver_to_json_string_udf = udf(conver_to_json_string, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527bbfb9-4313-4eda-886d-a9d8223b82a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = table_processor.dataframe\n",
    "\n",
    "columns = ['FechaOpen', 'FechaClick', 'Links', 'IPs', 'Navegadores', 'Plataformas']\n",
    "for column in columns:\n",
    "    df = df.withColumn(column, regexp_replace(col(column), 'unknown', ''))\n",
    "    df = df.withColumn(column, when(col(column) == '-', lit(None)).otherwise(col(column)))\n",
    "    df = df.withColumn(column, when(col(column) == '', lit(None)).otherwise(col(column)))\n",
    "\n",
    "columns = ['Links', 'IPs', 'Navegadores', 'Plataformas']\n",
    "for column in columns:\n",
    "    df = df.withColumn(column, conver_to_json_string_udf(col(column)))\n",
    "\n",
    "#FAKE ERRORS\n",
    "df = df.withColumn('Baja', when((col('email') == 'jhosebh_19@yahoo.com') &\n",
    "                                (col('baja') == 'SI') &\n",
    "                                (col('FechaEnvio') == '08/02/2013 18:30'), lit('YES')).otherwise(col('Baja')))\n",
    "\n",
    "df = df.withColumn('Clicks', when((col('email') == 'jhosebh_19@yahoo.com') &\n",
    "                                (col('FechaEnvio') == '08/02/2013 18:30'), lit(1000)).otherwise(col('Clicks')))\n",
    "\n",
    "df = df.withColumn('FechaEnvio', when((col('email') == 'irmis_20@yahoo.com') &\n",
    "                                (col('FechaEnvio') == '08/02/2013 18:30'), lit('2013-02-08 18:30')).otherwise(col('FechaEnvio')))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"row_temp_id\", IntegerType(), False),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"screen_code\", StringType(), False)\n",
    "])\n",
    "dataframe_errors = spark.createDataFrame([], schema)\n",
    "\n",
    "class AbstractScreenValidator(ABC):\n",
    "\n",
    "    def __init__(self, dataframe, column, dataframe_errors):\n",
    "        self.dataframe = dataframe\n",
    "        self.column = column\n",
    "        self.dataframe_errors = dataframe_errors\n",
    "        self.validation_code = self.get_validation_code()\n",
    "\n",
    "    @abstractmethod\n",
    "    def filter(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_validation_code(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        print('Validating column: ' + self.column + ' with screen ' + self.validation_code)\n",
    "        \n",
    "        fails = self.filter()\n",
    "        fails = fails.select('row_temp_id', self.column)\n",
    "        fails = fails.withColumnRenamed(self.column, 'value')\n",
    "        fails = fails.withColumn('screen_code', lit(self.validation_code))\n",
    "        fails = fails.withColumn('value', col('value').cast(StringType()))\n",
    "\n",
    "        self.dataframe_errors = self.dataframe_errors.union(fails)\n",
    "\n",
    "        fails_list = fails.select('row_temp_id').collect()\n",
    "        fails_list = [getattr(row, 'row_temp_id') for row in fails_list]\n",
    "\n",
    "        self.dataframe = self.dataframe.withColumn('audit_passed', when(col('row_temp_id').isin(fails_list), False).otherwise(col('audit_passed')))\n",
    "        self.dataframe = self.dataframe.withColumn(column, when(col('row_temp_id').isin(fails_list), lit(None)).otherwise(col(column)))\n",
    "\n",
    "        return self.dataframe, self.dataframe_errors\n",
    "\n",
    "class IsNotNullScreenValidator(AbstractScreenValidator):\n",
    "\n",
    "    def filter(self):\n",
    "        return self.dataframe.where(col(self.column).isNull()).select('row_temp_id', self.column)\n",
    "\n",
    "    def get_validation_code(self):\n",
    "        return 'is_not_null'\n",
    "    \n",
    "class IsInListScreenValidator(AbstractScreenValidator):\n",
    "\n",
    "    def set_allowed_values(self, allowed_values):\n",
    "        self.allowed_values = allowed_values\n",
    "\n",
    "    def filter(self):\n",
    "        return self.dataframe.where(~col(self.column).isin(self.allowed_values))\n",
    "\n",
    "    def get_validation_code(self):\n",
    "        return 'is_in_list'\n",
    "    \n",
    "class IsEmailFormatScreenValidator(AbstractScreenValidator):\n",
    "\n",
    "    def filter(self):\n",
    "        return self.dataframe.where(~regexp_extract(col(column), r'^.+@.+\\..+$', 0).cast('string').isNotNull())\n",
    "\n",
    "    def get_validation_code(self):\n",
    "        return 'is_email_format'\n",
    "    \n",
    "class IsInBoundsScreenValidator(AbstractScreenValidator):\n",
    "\n",
    "    def set_bounds(self, min_allowed, max_allowed):\n",
    "        self.min_allowed = min_allowed\n",
    "        self.max_allowed = max_allowed\n",
    "\n",
    "    def filter(self):\n",
    "        return self.dataframe.where((col(self.column) < self.min_allowed) | (col(self.column) > self.max_allowed))\n",
    "\n",
    "    def get_validation_code(self):\n",
    "        return 'is_in_bounds'\n",
    "\n",
    "class IsDateFormatScreenValidator(AbstractScreenValidator):\n",
    "\n",
    "    def set_format(self, format):\n",
    "        self.format = format\n",
    "\n",
    "    def filter(self):\n",
    "\n",
    "        fails = df.withColumn('temp', when((try_to_date(col(column), self.format).isNotNull()) |\n",
    "                                           (col(column).isNull()), True).otherwise(False))\n",
    "        return fails.where((col('temp') == False))\n",
    "    \n",
    "    def get_validation_code(self):\n",
    "        return 'is_date_format'\n",
    "\n",
    "#IS NOT NULL SCREEN\n",
    "for column in parsed_validations['is_not_null']:  \n",
    "\n",
    "    screen_validator = IsNotNullScreenValidator(df, column, dataframe_errors)\n",
    "    df, dataframe_errors = screen_validator.validate()\n",
    "\n",
    "#IS IN LIST SCREEN\n",
    "for column in parsed_validations['is_in_list']:\n",
    "\n",
    "    screen_validator = IsInListScreenValidator(df, column, dataframe_errors)\n",
    "    screen_validator.set_allowed_values(parsed_validations['is_in_list'][column])\n",
    "    df, dataframe_errors = screen_validator.validate()\n",
    "\n",
    "#IS EMAIL FORMAT\n",
    "for column in parsed_validations['is_email_format']:\n",
    "    \n",
    "    screen_validator = IsEmailFormatScreenValidator(df, column, dataframe_errors)\n",
    "    df, dataframe_errors = screen_validator.validate()\n",
    "\n",
    "#IS IN BOUNDS SCREEN\n",
    "for column in parsed_validations['is_in_bounds']:\n",
    "\n",
    "    screen_validator = IsInBoundsScreenValidator(df, column, dataframe_errors)\n",
    "    screen_validator.set_bounds(parsed_validations['is_in_bounds'][column]['min_allowed'], \n",
    "                                parsed_validations['is_in_bounds'][column]['max_allowed'])\n",
    "    df, dataframe_errors = screen_validator.validate()\n",
    "\n",
    "#IS DATE FORMAT SCREEN\n",
    "for column in parsed_validations['is_date_format']:\n",
    "\n",
    "    screen_validator = IsDateFormatScreenValidator(df, column, dataframe_errors)\n",
    "    screen_validator.set_format(parsed_validations['is_date_format'][column])\n",
    "    df, dataframe_errors = screen_validator.validate()\n",
    "\n",
    "df.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975cc87c-b7d6-4b53-a37d-3d112c86a08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subset = df.where(col('audit_passed') == False)\n",
    "print(subset.count())\n",
    "subset.show(3)\n",
    "\n",
    "print(dataframe_errors.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70137c62-d71b-4968-bbfe-473e339bf5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def screen_data(self):\n",
    "        print('Screening data...')\n",
    "\n",
    "        certificate_numbers = self.context['list_values']['certificate_numbers']\n",
    "        cie_identifiers = self.context['list_values']['cie_identifiers']\n",
    "        \n",
    "        self.facade_screens.setup(data=self.data, table_name='claims', identifier='claim_id')\n",
    "        \n",
    "        #CHECK NULL VALUES\n",
    "        columns = ['state', 'cie_id', 'diagnosis', 'incident_date', 'payments', 'coinsurance',\n",
    "                   'ivarec', 'deductible', 'incident_reason', 'cve_mes', 'month_cont_date', 'payment_type', 'provider', 'certificate_number']\n",
    "        for column in columns:\n",
    "            self.facade_screens.apply_screen_is_missing_value(column)\n",
    "\n",
    "        #CHECK UNIQUE VALUES\n",
    "        self.facade_screens.apply_screen_is_not_unique('claim_id')\n",
    "\n",
    "        #CHECK NOT DIGIT STRING VALUES\n",
    "        self.facade_screens.apply_screen_is_not_digit_string('certificate_number', 6)\n",
    "\n",
    "        #CHECK NOT DATE FORMAT VALUES\n",
    "        self.facade_screens.apply_screen_is_not_date_format('incident_date', '%d/%m/%Y')\n",
    "        self.facade_screens.apply_screen_is_not_date_format('payment_date', '%d/%m/%Y')\n",
    "        self.facade_screens.apply_screen_is_not_date_format('first_expense_date', '%d/%m/%Y')\n",
    "        self.facade_screens.apply_screen_is_not_date_format('month_cont_date', '%d/%m/%Y')\n",
    "\n",
    "        self.data['incident_date'] = pd.to_datetime(self.data['incident_date'], format='%d/%m/%Y')\n",
    "        self.data['payment_date'] = pd.to_datetime(self.data['payment_date'], format='%d/%m/%Y')\n",
    "        self.data['first_expense_date'] = pd.to_datetime(self.data['first_expense_date'], format='%d/%m/%Y')\n",
    "        self.data['month_cont_date'] = pd.to_datetime(self.data['month_cont_date'], format='%d/%m/%Y')\n",
    "\n",
    "        #CHECK OUT OF BOUNDS VALUES\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('incident_date', pd.Timestamp('2020-01-01'), pd.Timestamp('2030-12-31'))\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('payment_date', pd.Timestamp('2020-01-01'), pd.Timestamp('2030-12-31'))\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('first_expense_date', pd.Timestamp('2020-01-01'), pd.Timestamp('2030-12-31'))\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('month_cont_date', pd.Timestamp('2020-01-01'), pd.Timestamp('2030-12-31'))\n",
    "\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('ocurrido', -1000000, 10000000)\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('payments', 0, 10000000)\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('coinsurance', 0, 1000000)\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('ivarec', 0, 1000000)\n",
    "        self.facade_screens.apply_screen_is_out_of_bounds_value('deductible', 0, 1000000)\n",
    "\n",
    "        #CHECK CRONOLOGICAL ORDER\n",
    "        self.facade_screens.apply_screen_is_lower_than('month_cont_date', 'payment_date')\n",
    "        self.facade_screens.apply_screen_is_lower_than('payment_date', 'first_expense_date')\n",
    "        self.facade_screens.apply_screen_is_lower_than('first_expense_date', 'incident_date')\n",
    "\n",
    "        self.data['incident_date_id'] = self.data['incident_date'].dt.strftime('%Y%m%d')\n",
    "        self.data['payment_date_id'] = self.data['payment_date'].dt.strftime('%Y%m%d')\n",
    "        self.data['first_expense_date_id'] = self.data['first_expense_date'].dt.strftime('%Y%m%d')\n",
    "        self.data['month_cont_date_id'] = self.data['month_cont_date'].dt.strftime('%Y%m%d')\n",
    "\n",
    "        self.data['incident_date_id'] = self.data['incident_date_id'].fillna('-2').astype(int)\n",
    "        self.data['payment_date_id'] = self.data['payment_date_id'].fillna('-2').astype(int)\n",
    "        self.data['first_expense_date_id'] = self.data['first_expense_date_id'].fillna('-2').astype(int)\n",
    "        self.data['month_cont_date_id'] = self.data['month_cont_date_id'].fillna('-2').astype(int)\n",
    "\n",
    "        #CHECK OUT OF LIST VALUES\n",
    "        self.facade_screens.apply_screen_is_out_of_list_value('certificate_number', certificate_numbers)\n",
    "        self.facade_screens.apply_screen_is_out_of_list_value('incident_reason', ['Enfermedad', 'Accidente'])\n",
    "        self.facade_screens.apply_screen_is_out_of_list_value('payment_type', ['Pago Directo'])\n",
    "        self.facade_screens.apply_screen_is_out_of_list_value('cie_id', cie_identifiers)\n",
    "\n",
    "        self.errors = self.facade_screens.get_error_events_detail()\n",
    "        self.data = self.data.drop(columns=['__screen__'])\n",
    "\n",
    "        #ADD AUDIT FACT COLUMN\n",
    "        self.data['audit_passed'] = 'SÃ­'\n",
    "        audit_dim_assembler = AuditDimensionAssembler(self.errors, 'claims')\n",
    "        unsolved_rows = audit_dim_assembler.get_unsolved_rows()\n",
    "        self.data.loc[self.data['claim_id'].isin(unsolved_rows), 'audit_passed'] = 'No'\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_to_audit",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

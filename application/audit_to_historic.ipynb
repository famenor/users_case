{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3abf089-5429-430e-90e9-0474a4cd0b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864247dc-a92b-484d-b84c-704ad4efe347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from lib.template.audit_to_historic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ad956c-517e-458d-98e4-364fd46b617b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rundate = '20130208_000000'\n",
    "\n",
    "domain = 'domain'\n",
    "environment = 'dev'\n",
    "\n",
    "schema_name = 'silver_analytics'\n",
    "catalog_name = domain + '_' + environment\n",
    "\n",
    "table_name = 'historic_visitas'\n",
    "\n",
    "print('Processing ' + table_name)\n",
    "factory_audit_to_historic = FactoryAuditToHistoric()\n",
    "factory_audit_to_historic.create()\n",
    "\n",
    "table_processor = AuditToHistoricTemplate(catalog_name=catalog_name, schema_name=schema_name, table_name=table_name)  \n",
    "table_processor.set_component_factory(factory_audit_to_historic) \n",
    "table_processor.process()\n",
    "\n",
    "print('')\n",
    "#print(table_processor.metadata)\n",
    "print(table_processor.metrics)\n",
    "print('')\n",
    "table_processor.dataframe.show(3)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b0bb44-b842-4ec1-b61c-e2eb1ab6cfa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = table_processor.dataframe\n",
    "print(df.count())\n",
    "print(df.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f437750-2ae2-494d-8004-e46e8badddf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#EXTRACT OR CREATE HISTORIC TABLE\n",
    "primary_key = 'Email'\n",
    "valid_from = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "valid_to = datetime.datetime.strptime('2200-01-01 00:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "try:\n",
    "    original_dataframe = spark.read.table(f'{table_processor.metadata['catalog_name']}.{table_processor.metadata['schema_name']}.{table_processor.metadata['table_name']}')\n",
    "    original_dataframe.count()\n",
    "\n",
    "except:\n",
    "    print('excepcion')\n",
    "    new_original_dataframe = table_processor.dataframe\n",
    "    new_original_dataframe = new_original_dataframe.withColumn('is_current', lit(True))\n",
    "    new_original_dataframe = new_original_dataframe.withColumn('valid_from', lit(valid_from))\n",
    "    new_original_dataframe = new_original_dataframe.withColumn('valid_to', lit(valid_to))\n",
    "\n",
    "    new_original_dataframe.write.format('delta').mode('overwrite').saveAsTable(f'{table_processor.metadata['catalog_name']}.{table_processor.metadata['schema_name']}.{table_processor.metadata['table_name']}')\n",
    "\n",
    "#READ CURRENT HISTORIC FILE\n",
    "array_history_columns = []\n",
    "array_source_columns = []\n",
    "\n",
    "for column in table_processor.metadata['field_data']:\n",
    "    if table_processor.metadata['field_data'][column]['track_changes'] == True:\n",
    "        array_history_columns.append(column)\n",
    "        array_source_columns.append('source_' + column)\n",
    "\n",
    "original_dataframe = spark.read.table(f'{table_processor.metadata['catalog_name']}.{table_processor.metadata['schema_name']}.{table_processor.metadata['table_name']}')\n",
    "table_processor.metrics['initial_rows'] = original_dataframe.count()\n",
    "print(table_processor.metrics['initial_rows'])\n",
    "\n",
    "\n",
    "new_dataframe = table_processor.dataframe.select([col(column).alias('source_' + column) for column in table_processor.dataframe.columns])\n",
    "\n",
    "new_dataframe_2 = new_dataframe\n",
    "new_dataframe_2 = new_dataframe_2.withColumn('source_is_current', lit(True))\n",
    "new_dataframe_2 = new_dataframe_2.withColumn('source_valid_from', lit(valid_from))\n",
    "new_dataframe_2 = new_dataframe_2.withColumn('source_valid_to', lit(valid_to))\n",
    "\n",
    "source_primary_key = 'source_' + primary_key\n",
    "merge_dataframe = original_dataframe.join(new_dataframe_2, (new_dataframe_2[source_primary_key] == original_dataframe[primary_key]), how='fullouter') \n",
    "\n",
    "merge_dataframe = merge_dataframe.withColumn('concat_ws', concat_ws('+', *array_history_columns))\n",
    "merge_dataframe = merge_dataframe.withColumn('concat_ws_source', concat_ws('+', *array_source_columns))\n",
    "\n",
    "merge_dataframe = merge_dataframe.withColumn('action', when(concat_ws('+', *array_history_columns) == \\\n",
    "                                                            concat_ws('+', *array_source_columns), 'NO_ACTION')\n",
    "                                                .when(merge_dataframe['is_current'] == False, 'NO ACTION')\n",
    "                                                .when(merge_dataframe[source_primary_key].isNull() & merge_dataframe.is_current, 'DELETE')\n",
    "                                                .when(merge_dataframe[primary_key].isNull(), 'INSERT')\n",
    "                                                .otherwise('UPDATE'))\n",
    "\n",
    "#test = merge_dataframe.filter(col('source_Email') == 'migue_235@yahoo.com')\n",
    "#test.show(truncate=False)\n",
    "\n",
    "print('Rows with no action: ' + str(merge_dataframe.filter(col('action') == 'NO_ACTION').count()))\n",
    "print('Rows to insert: ' + str(merge_dataframe.filter(col('action') == 'INSERT').count()))\n",
    "print('Rows to update: ' + str(merge_dataframe.filter(col('action') == 'UPDATE').count()))\n",
    "print('Rows to delete: ' + str(merge_dataframe.filter(col('action') == 'DELETE').count()))\n",
    "\n",
    "array_history_columns = array_history_columns + ['is_current', 'valid_from', 'valid_to']\n",
    "array_source_columns = array_source_columns + ['source_is_current', 'source_valid_from', 'source_valid_to']\n",
    "\n",
    "#RECORDS WITH NO ACTION\n",
    "df_merge_part_01 = merge_dataframe.filter(merge_dataframe.action == 'NO_ACTION').select(array_history_columns)\n",
    "\n",
    "#RECORDS TO INSERT\n",
    "df_merge_part_02A = merge_dataframe.filter(merge_dataframe.action == 'INSERT').select(array_source_columns)\n",
    "df_merge_part_02B = df_merge_part_02A.select([col(column).alias(column.replace('source_', '')) for column in df_merge_part_02A.columns])\n",
    "\n",
    "#RECORDS TO DELETE\n",
    "df_merge_part_03 = merge_dataframe.filter(merge_dataframe.action == 'DELETE').select(array_history_columns)\n",
    "df_merge_part_03 = df_merge_part_03.withColumn('is_current', lit(False))\n",
    "df_merge_part_03 = df_merge_part_03.withColumn('valid_to', lit(valid_to))\n",
    "\n",
    "#RECORDS TO EXPIRE AND INSERT\n",
    "df_merge_part_04A = merge_dataframe.filter(merge_dataframe.action == 'UPDATE').select(array_source_columns)\n",
    "df_merge_part_04B = df_merge_part_04A.select([col(column).alias(column.replace('source_', '')) for column in df_merge_part_02A.columns])\n",
    "\n",
    "df_merge_part_04C = merge_dataframe.filter(merge_dataframe.action == 'UPDATE')\n",
    "df_merge_part_04C = df_merge_part_04C.withColumn('valid_to', merge_dataframe['source_valid_from'])\n",
    "df_merge_part_04C = df_merge_part_04C.withColumn('is_current', lit(False))\n",
    "df_merge_part_04C = df_merge_part_04C.select(array_history_columns)\n",
    "\n",
    "#print('>>>>> 01')\n",
    "#print(df_merge_part_01.count())\n",
    "#df_merge_part_01.show(3)\n",
    "\n",
    "#print('>>>>> 02A')\n",
    "#print(df_merge_part_02A.count())\n",
    "#df_merge_part_02A.show(3)\n",
    "\n",
    "#print('>>>>> 03')\n",
    "#print(df_merge_part_03.count())\n",
    "#df_merge_part_03.show(3)\n",
    "\n",
    "#print('>>>>> 04B')\n",
    "#print(df_merge_part_04B.count())\n",
    "#df_merge_part_04A.show(3)\n",
    "\n",
    "#print('>>>>> 04C')\n",
    "#print(df_merge_part_04C.count())\n",
    "#df_merge_part_04C.show(3)\n",
    "\n",
    "#UNION\n",
    "dataframe_historic = df_merge_part_01.unionAll(df_merge_part_02B).unionAll(df_merge_part_03).unionAll(df_merge_part_04B).unionAll(df_merge_part_04C)\n",
    "\n",
    "print('>>>>> HIST')\n",
    "count = dataframe_historic.count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b337ba-a7fe-4415-9bff-83d33862fae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_historic.filter(col('Email') == 'migue_235@yahoo.com').show(10)\n",
    "dataframe_historic.filter(col('is_current') == False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397b229b-2597-4f05-b097-f71de17b1a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE domain_dev.silver_analytics.historic_visitas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c2068b-c17c-454a-b092-8308132949a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql('SELECT * FROM governance_prod.metrics.ingestions')\n",
    "print(result.show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e52966-3cc1-4cf2-9d55-0f649bcec531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql('SELECT * FROM domain_dev.silver_analytics.historic_visitas')\n",
    "print(result.count())\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f299311f-386a-4d5d-b065-58cf165c7fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = spark.sql(\"SELECT * FROM domain_dev.silver_analytics.audit_visitas WHERE metadata_batch_id='8'\")\n",
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fea83088-411d-4c34-9170-b97e689374cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "samanta.llaguno@a"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "audit_to_historic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

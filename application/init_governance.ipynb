{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6c5ee5-a7af-427f-a6fb-b359e19f9b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql('DROP CATALOG IF EXISTS governance_prod CASCADE')\n",
    "#spark.sql('DROP CATALOG IF EXISTS domain_dev CASCADE')\n",
    "#spark.sql('DROP CATALOG IF EXISTS domain_prod CASCADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96af8181-8875-4187-a4b5-82715a949100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('CREATE CATALOG IF NOT EXISTS governance_prod')\n",
    "spark.sql('CREATE CATALOG IF NOT EXISTS domain_dev')\n",
    "spark.sql('CREATE CATALOG IF NOT EXISTS domain_prod')\n",
    "\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS governance_prod.metadata')\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS governance_prod.metrics')\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS domain_dev.bronze_analytics')\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS domain_dev.silver_analytics')\n",
    "spark.sql('CREATE SCHEMA IF NOT EXISTS domain_dev.gold_analytics')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea253d3-3b8e-4324-acd6-4d7bbb579a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GOVERNANCE - METADATA - CATALOGS\n",
    "array_id = [1, 2, 3]\n",
    "array_name = ['governance_prod', 'domain_dev', 'domain_prod']\n",
    "array_domain = ['governance', 'domain', 'domain']\n",
    "array_environment = ['prod', 'dev', 'prod']\n",
    "array_description = ['Governance data and Metadata', 'Development environment for custom domain', 'Production environment for custom domain']\n",
    "array_location = ['Databricks'] * 3\n",
    "array_created_at = ['2025-11-27 00:00:00'] * 3\n",
    "array_owner = ['armando.n90@gmail.com', 'armando.n90@gmail.com', 'armando.n90@gmail.com']\n",
    "\n",
    "columns_comments = {\n",
    "    \"catalog_id\": \"Identifier of the catalog\",\n",
    "    \"catalog_name\": \"Name of the catalog\",\n",
    "    \"domain\": \"Domain of the catalog\",\n",
    "    \"environment\": \"Environment of the catalog among dev and prod\",\n",
    "    \"description\": \"Description of the catalog\",\n",
    "    \"location\": \"Location of the catalog\",\n",
    "    \"created_at\": \"Date of creation of the catalog\",\n",
    "    \"owner\": \"Owner of the catalog\",\n",
    "}\n",
    "\n",
    "metadata_catalog = spark.createDataFrame(data = list(zip(array_id, array_name, array_domain, array_environment, array_description, array_location, array_created_at, array_owner)), schema=['catalog_id', 'catalog_name', 'domain', 'environment', 'description', 'location', 'created_at', 'owner'])\n",
    "\n",
    "print(metadata_catalog.count())\n",
    "metadata_catalog.show(10)\n",
    "\n",
    "metadata_catalog.write.format('delta').mode('overwrite').saveAsTable('governance_prod.metadata.catalogs')\n",
    "\n",
    "for column, comment in columns_comments.items():\n",
    "    spark.sql(f\"ALTER TABLE governance_prod.metadata.catalogs ALTER COLUMN {column} COMMENT '{comment}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185e9f3e-d3a8-4683-9067-a87d4e7bb975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GOVERNANCE - METADATA - SCHEMAS\n",
    "array_id = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "array_catalog_id = [1, 1, 2, 2, 2, 3, 3, 3]\n",
    "array_name = ['metadata', 'metrics', \n",
    "              'bronze_analytics', 'silver_analytics', 'gold_analytics', \n",
    "              'bronze_analytics', 'silver_analytics', 'gold_analytics']\n",
    "array_description = ['Metadata', 'Metrics', \n",
    "                     'Bronze medallion for analytics', 'Silver medallion for analytics', 'Gold medallion for analytics', 'Bronze medallion for analytics', 'Silver medallion for analytics', 'Gold medallion for analytics']\n",
    "array_created_at = ['2025-11-27 00:00:00'] * 8\n",
    "array_owner = ['armando.n90@gmail.com'] * 8\n",
    "\n",
    "columns_comments = {\n",
    "    \"schema_id\": \"Identifier of the schema\",\n",
    "    \"catalog_id\": \"Identifier of the catalog\",\n",
    "    \"schema_name\": \"Name of the schema\",\n",
    "    \"description\": \"Description of the schema\",\n",
    "    \"created_at\": \"Date of creation of the schema\",\n",
    "    \"owner\": \"Owner of the schema\",\n",
    "}\n",
    "\n",
    "metadata_table = spark.createDataFrame(data = list(zip(array_id, array_catalog_id, array_name,  array_description, array_created_at, array_owner)), schema=['schema_id', 'catalog_id', 'schema_name', 'description', 'created_at', 'owner'])\n",
    "\n",
    "print(metadata_table.count())\n",
    "metadata_table.show(10)\n",
    "\n",
    "metadata_table.write.format('delta').mode('overwrite').saveAsTable('governance_prod.metadata.schemas')\n",
    "\n",
    "for column, comment in columns_comments.items():\n",
    "    spark.sql(f\"ALTER TABLE governance_prod.metadata.schemas ALTER COLUMN {column} COMMENT '{comment}'\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56c27e1-c8ed-4491-9076-c052418dabe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GOVERNANCE - METADATA - TABLES\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS governance_prod.metadata.tables \n",
    "             (\n",
    "                table_id INTEGER COMMENT 'Identifier of the table', \n",
    "                schema_id INTEGER COMMENT 'Identifier of the schema', \n",
    "                table_name STRING COMMENT 'Name of the table', \n",
    "                etl_module STRING COMMENT 'Name of the ETL module that ingests the table', \n",
    "                write_mode STRING COMMENT 'Write mode of the table among overwrite_partition, overwrite, merge and cdc', \n",
    "                quality STRING COMMENT 'Quality of the table among bronze, silver and gold', \n",
    "                table_type STRING COMMENT 'Type of the table among table, view and materialized', \n",
    "                version INTEGER COMMENT 'Version of the table', \n",
    "                current_flag BOOLEAN COMMENT 'Flag to indicate if the record is the current one',\n",
    "                valid_from TIMESTAMP COMMENT 'Date of validity of the record',\n",
    "                valid_to TIMESTAMP COMMENT 'Date of invalidity of the record', \n",
    "                description STRING COMMENT 'Description of the table',\n",
    "                owner STRING COMMENT 'Owner of the table',\n",
    "                retention_policy STRING COMMENT 'Retention policy of the table among permanent and temporary'\n",
    "             )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ba1556-ddf3-4dde-a9ea-a90bbee34894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GOVERNANCE - METADATA - TABLES DETAIL\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS governance_prod.metadata.tables_detail \n",
    "             (\n",
    "                table_id INTEGER COMMENT 'Identifier of the table', \n",
    "                column_id INTEGER COMMENT 'Identifier of the column', \n",
    "                column_name STRING COMMENT 'Name of the column', \n",
    "                rename_from STRING COMMENT 'Name of the column on the prevous layer',\n",
    "                data_type STRING COMMENT 'Data type of the column', \n",
    "                ordinal_position INTEGER COMMENT 'Ordinal position of the column',\n",
    "                is_primary_key BOOLEAN COMMENT 'Flag to indicate if the column is a primary key',\n",
    "                is_nullable BOOLEAN COMMENT 'Flag to indicate if the column is nullable',\n",
    "                is_partition BOOLEAN COMMENT 'Flag to indicate if the column is a partition',\n",
    "                is_pii BOOLEAN COMMENT 'Flag to indicate if the column is a personal identification information',\n",
    "                validations STRING COMMENT 'Validation in JSON string',\n",
    "                comment STRING COMMENT 'Comment of the column'\n",
    "             )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512bd3b9-4e62-43b7-897a-fd71a59da0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GOVERNANCE - METRICS - INGESTIONS\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS governance_prod.metrics.ingestions (table_id INTEGER, \n",
    "                catalog_name STRING, schema_name STRING, table_name STRING, batch_id STRING, loaded_at TIMESTAMP, etl_module STRING, \n",
    "                write_mode STRING, initial_rows LONG, final_rows LONG, accumulated_rows LONG, quality STRING)\"\"\")\n",
    "\n",
    "columns_comments = {\n",
    "    \"table_id\": \"Identifier of the table\",\n",
    "    \"catalog_name\": \"Name of the catalog\",\n",
    "    \"schema_name\": \"Name of the schema\",\n",
    "    \"table_name\": \"Name of the table\",\n",
    "    \"batch_id\": \"Identifier of the ingested batch\",\n",
    "    \"loaded_at\": \"Date of loading\",\n",
    "    \"etl_module\": \"Name of the ETL module that created the table\",\n",
    "    \"write_mode\": \"Write mode of the table among overwrite_partition, overwrite, merge and cdc\",\n",
    "    \"initial_rows\": \"Number of rows in the batch when the table was read\",\n",
    "    \"final_rows\": \"Number of rows in the batch when the the table was written\",\n",
    "    \"accumulated_rows\": \"Number of rows accumulated in the table after the ingestion\",\n",
    "    \"quality\": \"Quality of the table among bronze, silver and gold\"\n",
    "}\n",
    "\n",
    "for column, comment in columns_comments.items():\n",
    "    spark.sql(f\"ALTER TABLE governance_prod.metrics.ingestions ALTER COLUMN {column} COMMENT '{comment}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab5c529-ed94-4842-9bbf-1b12e0536ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#GOVERNANCE - METRICS - EVENT ERRORS\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS governance_prod.metrics.event_errors (\n",
    "    error_event_id STRING, batch_id STRING, screen_code STRING, catalog_name STRING, schema_name STRING,\n",
    "    table_name STRING, column_name STRING, record_identifier STRING, original_value STRING, replaced_value STRING,\n",
    "    error_condition STRING\n",
    ")\"\"\")\n",
    "\n",
    "columns_comments = {\n",
    "    \"error_event_id\": \"Unique identifier of the error event\",\n",
    "    \"batch_id\": \"Unique identifier of the batch\",\n",
    "    \"screen_code\": \"Unique code of the screen\",\n",
    "    \"catalog_name\": \"Name of the catalog\",\n",
    "    \"schema_name\": \"Name of the schema\",\n",
    "    \"table_name\": \"Name of the table\",\n",
    "    \"column_name\": \"Name of the column\",\n",
    "    \"record_identifier\": \"Unique identifier of the record\",\n",
    "    \"original_value\": \"Original value of the record\",\n",
    "    \"replaced_value\": \"Replaced value of the record\",\n",
    "    \"error_condition\": \"Error condition of the record\"\n",
    "}\n",
    "\n",
    "for column, comment in columns_comments.items():\n",
    "    spark.sql(f\"ALTER TABLE governance_prod.metrics.event_errors ALTER COLUMN {column} COMMENT '{comment}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9047f06e-4bdf-449f-b013-8408845c50e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#test = spark.sql('SELECT * FROM governance_prod.metadata.catalogs')\n",
    "#test = spark.sql('SELECT * FROM governance_prod.metadata.schemas')\n",
    "#test = spark.sql('SELECT * FROM governance_prod.metadata.tables')\n",
    "#test = spark.sql('SELECT * FROM governance_prod.metadata.tables_detail')\n",
    "#test = spark.sql('SELECT * FROM governance_prod.metrics.ingestions')\n",
    "test = spark.sql('SELECT * FROM governance_prod.metrics.event_errors')\n",
    "test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce9e72d-91f0-4830-821c-cab5fc90126f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql('DELETE FROM governance_prod.metrics.event_errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53b8166c-bc3f-4531-82f5-21bad646b945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "init_governance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
